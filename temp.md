# Bhanzu
## Payments app
1. Can you walk me through the **challenges you faced during this integration process**? How did you overcome these challenges, especially considering international payment compliance and user experience?
- Due to clients in various countries, we had to Integrate a lot of **different Payment Vendors** (eg `Juspay`, `Razorpay`, `Stripe`, `PayU`, `Tabby`). For this we took help from the Ops Team to understand the role of different Vendors
- Working with **GST** was a bit tricky, from ordering to receipt generation etc. For this we closely worked with the Finance Team>
- I specifically remember one istance where we found that not all vendors support the same **unique-id format**. We had begun with `uuid4`, but as more and more Payment Vendors kept on increasing, it kept on changing. Finally, we settled with a short 13-char alphanumeric string.
- **Communication between different microservices** was very crucial, for this we used AWS SQS.

2. What was the **measurable impact** of this solution on the business or user satisfaction?
- **Final Solution**: Automating the Payment flow for the user. Before this, the Ops team used to create the payment links manually.
- Impact on **Ops Team**: Automation helped with their task. They could utilize the time in something else.
- Impact on **Finance Team**: With all the data routing through our services, we could use this data for various use. From `auditing` to `monitoring`, to `Aggregation` and `Report Generation`, it was easy for them
## Auto QC
- First let me explain about the product a bit
    - **Picked from Hackathon:** We participated in a Company-wide 2-day Hackathon, where we won 2nd prized. And later that week, we were asked to pull this project out in production.
    - **Metrics:** There were around 40-45 metrics on which the QC team used to QC the video lecture. We picked out around 25 of those that could be answered using transcript (eg was the teacher rude or not? was the teacher giving equal amount of time to each of the students or not?).
    - We used **OpenAI Whisper** (finally, after trying various tools) for **transcribing**.

1. What **challenges** did you face in implementing AI for video QC and feedback?
- **Picking out the Metrics** in itself was a challenge. With the advancement of Gen AI, a lot of questions can be answered. Picking out the ones where the AI would be close to human mind was a challenge and required various iterations.
- **Prompt Engineering** was a task. We went through a course by Andrew NG, to help us come up with better prompts.
- **Trying various transcribing tools** and finalizing one was a headache. it had to be manually. we tried Assembly AI (best results, but not good financially for the organization), IBM Watson, Open AI Whisper, Google Long, Google Chirp etc. Finally, we went with Whisper because of decent result and Open AI Credits.

2. How did you **ensure the AI-generated feedback** was **accurate**, **actionable**, and **aligned with the trainers’ needs**?
- We had a **QC team** in the organization. And they assigned us 2 people, who we were constantly in touch with. They helped us with some of the **manual tasks** (like verifying AI-generated response, validating the response agains the human-generated one)
- With the constant **feedback by the QC team**, we would **enhance our prompts** to better fit the need of the organization.
- We went with a **rating system to rate the response generated by the AI** out of 10. After each iteration, the QC tea would be given a random sample and AI-generated response (eg 3 old videos, and 2 new). Based on these ratings, we would figure out if the AI was up to the org's standard or not.

3. Can you share an example of **how this solution improved** trainer **performance** or **operational efficiency**?
- There were various scenarios where the **biweekly feedback** would improve trainers' performance.
- With a **fear of constantly being monitored**, the trainers would improve a lot. For a long time, we had both AI and QC team perform the QC, and the result was clear.
- Majority of the **QC team was now desolved**. They were transferred to other teams (eg curriculum, ops) based on their interest and capabilities.
- **Manual QC** would result in only **15-18%** of the videos being QC'd whereas we would QC around **70-80%** of the videos.
- The project was estimated to **increase the revenue by around 4%**.


# AI-fluence
## 3rd Party API Integration
1. What specific **challenges** did you face while designing this architecture, particularly in integrating multiple third-party APIs (like Google, Slack, Twitter, etc.)?
- This was a **monolithic Architecture**, so the projects were really simple.
- We integrated various third-party APIs. While some vendors had really good **Documentation** (making the integration easy), others were not up-to-the-mark. Going through the Documentation of cor for example, took some time.
- Various tasks took a bit longer than expected eg, following proper structuring in the Google Drive.
- Talking about the Architecture, we made a Decision of going with Golang, because of the **Go Routines** and our requirement of working with various parallel tasks.
- For various tasks we needed a **real-time updates** eg file uploaded, or status of the post etc. For this we initially thought of a web socket, but later on moved to Long Polling.

2. How did you ensure **scalability** and **maintainability** of the architecture for future feature enhancements?
- While we were working with a limited set of nano-influencers, we **need not worry about scalability** most of the time. That's why we implemented **Monolithic Architecture**. However, we had segregated the code well enough to divide it into micro-services when time comes.
- We had integrated **Status manager** to track the Status of the post. there was a flowchart to be followed that would contain the rules regarding the status. Eg a `completed` status can not be marked as `pending` again.
- With the scale we had, there was no requirement of multiple instances of our Backend system. However, we did have **multiple replicas of our Mongo DB Cluster**.

3. Can you share any **significant technical decisions** you made that directly impacted the product’s performance or user satisfaction?
- One example is that of using Golang because of its **Go Routines** fulfilling our requirements.
- We went with **Docker**, **Kubernetes** and **AWS EKS** for our deployments, with **CI/CD** pipeline trigerred using **GitLab Actions**. We went with **GitLab** because of its favorable instance towards CI/CD Pipelines.
- For scalability issue, we create a **separate client-facing app**. It was a different monolith product.
## Influencer-facing and Brand-facing app
1. What were the **key differences in the requirements or design considerations** between the influencer-facing and brand-facing apps?
- The **Influencer-facing app was visually more appealing**. People were used to social media, so giving them a sense of visually-appealing app was really very important. Wheareas, visual appeal was not such a big priority for brand-facing app.
- **Influencer-facing app was made keeping latency in mind**. Most of our users were using the app/website on mobile, even from poor countries where data speed was low and data cost was high. So, we needed to make the app fast and consuming less data. Whereas that was not the priority for brand-facing app.
- **Brand-facing app had a lot of async programming** because of report generation and aggregation, whereas that was not the case with influencer-facing app.

2. How did you **address any conflicts or overlaps in their functionalities**?
- Both the apps, even though they belonged to the same domain and organization, had a **lot of difference in terms of requirement**. 
- As also mentioned during the previous question, the **brand-facing** app was supposed to be **low-data**, **low-latency**, **visually-appealing**, whereas the **influencer-facing** app was supposed to be **report-generation based**, **highly async**.
- Most of the decisions were based on these requirement. eg choosing **Golang for go routines and async programming**, **react for small components in the front-end**, **sql for transactions**, etc.

3. Can you provide an **example of a specific feature where user feedback significantly influenced the final implementation**?
- We were highly worried about the **influencers' disinterest regarding draft posts in the social media**. For this, we came up with a solution where we **showed each of the draft post in the same way as it would be seen on the social media**. For example, for a campaign to be run on Twitter, when a user enters a draft post they get a look and feel of that of how it would look like on Twitter. We used an external library for that.

# GE
## UI Components
1. What **challenges** did you face while designing a system that is both **adaptable** and **user-friendly**?
- There used to be **constant sittings with various set of stakeholders** to understand their requirements.
- The end product was **highly configurable UI components** with proper default values set.
- To make the UI components adaptable and user-friendly, we would encourage the stakeholder to raise **issues in our GitHub Repo**. And even though the priority would be decided later on in the meeting, someone from the team would respond to the issue and add proper Github flags.

1. How did you ensure **consistency** across different components while maintaining **flexibility for customization**?
- With **default values set**, users would generally opt for configuring only the part that were required.
- There was a **proper Documentation** that was maintained not only when the features are released, but also for on-going features. This Documentation would not only contain how to best use the UI Component, but also the best practices and guidelines.

1. Can you share an example of a **specific component or tool you developed** and how it positively impacted the overall system or user experience?
- We created various components. They were generally just a **wrapper around Material Component**, but sometimes stricter and sometimes more flexible (depending upon the need of the stakeholders).
- I remember we created a **wrapper around `ag-grid` for our table need**s. This gave the user a lot of functionalities that were missing from `ag-grid` eg multi-valued columns, and filtering them; sorting multiple columns at once; responsiveness of the table; etc


